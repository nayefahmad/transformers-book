{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Retrieval-augmented generation with LangChain\n\n## References: \n1. [LangChain docs - question-answering](https://python.langchain.com/docs/use_cases/question_answering/)\n1. [Medium/the-techlife - Using HF and Cohere models](https://medium.com/the-techlife/using-huggingface-openai-and-cohere-models-with-langchain-db57af14ac5b)\n1. [Lightning AI - Comparing different language models for question-answering](https://lightning.ai/pages/community/community-discussions/the-ultimate-battle-of-language-models-lit-llama-vs-gpt3.5-vs-bloom-vs/)","metadata":{}},{"cell_type":"code","source":"!pip install langchain\n# !pip install openai\n!pip install sentence_transformers\n# !pip install chromadb\n!pip install llama-cpp-python","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-08-16T19:44:09.243431Z","iopub.execute_input":"2023-08-16T19:44:09.243829Z","iopub.status.idle":"2023-08-16T19:45:30.212929Z","shell.execute_reply.started":"2023-08-16T19:44:09.243800Z","shell.execute_reply":"2023-08-16T19:45:30.211398Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (0.0.266)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.17)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.8.4)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.2)\nRequirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.5.9)\nRequirement already satisfied: langsmith<0.1.0,>=0.0.21 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.0.23)\nRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.8.4)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.23.5)\nRequirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.2.4)\nRequirement already satisfied: pydantic<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.10.9)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\nRequirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\nRequirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.6.3)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.5.7)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: sentence_transformers in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.30.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.65.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.0.0+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.15.1+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.1)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting llama-cpp-python\n  Downloading llama_cpp_python-0.1.77.tar.gz (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (4.6.3)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama-cpp-python) (1.23.5)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.77-cp310-cp310-linux_x86_64.whl size=279058 sha256=5406668dac6e3676762d560b67c8779fd0a0c3f0d4b36913403bda7a0303fef4\n  Stored in directory: /root/.cache/pip/wheels/aa/ed/39/87f2ad350dbbf13b600ac744899186b8647c5323c62e2bb348\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.1 llama-cpp-python-0.1.77\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain import HuggingFaceHub\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.vectorstores import FAISS, Chroma\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain import HuggingFacePipeline\nfrom sentence_transformers import SentenceTransformer","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:38:28.061833Z","iopub.execute_input":"2023-08-16T19:38:28.062563Z","iopub.status.idle":"2023-08-16T19:38:38.187001Z","shell.execute_reply.started":"2023-08-16T19:38:28.062519Z","shell.execute_reply":"2023-08-16T19:38:38.185782Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain import PromptTemplate, LLMChain\nfrom langchain.llms import HuggingFacePipeline\nfrom sentence_transformers import SentenceTransformer, util\nfrom transformers import pipeline\n\n# model = SentenceTransformer('all-MiniLM-L6-v2')\nmodel = pipeline(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n#Sentences are encoded by calling model.encode()\nemb1 = model.encode(\"This is a red cat with a hat.\")\nemb2 = model.encode(\"Have you seen my red cat?\")\n\ncos_sim = util.cos_sim(emb1, emb2)\nprint(\"Cosine-Similarity:\", cos_sim)\n\ntemplate = PromptTemplate(input_variables=[\"input\"], template=\"{input}\")\nchain = LLMChain(llm=model, verbose=True, prompt=template)\nchain(\"What is the meaning of life?\")","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:41:17.983471Z","iopub.execute_input":"2023-08-16T19:41:17.983901Z","iopub.status.idle":"2023-08-16T19:41:21.385953Z","shell.execute_reply.started":"2023-08-16T19:41:17.983869Z","shell.execute_reply":"2023-08-16T19:41:21.384358Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29de5e2975144c4fb5d070eb4ca25e6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m7\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mtransformers\u001b[0m \u001b[94mimport\u001b[0m pipeline                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m# model = SentenceTransformer('all-MiniLM-L6-v2')\u001b[0m                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 7 model = pipeline(model=\u001b[33m\"\u001b[0m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[0m\u001b[33m\"\u001b[0m)                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m#Sentences are encoded by calling model.encode()\u001b[0m                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0memb1 = model.encode(\u001b[33m\"\u001b[0m\u001b[33mThis is a red cat with a hat.\u001b[0m\u001b[33m\"\u001b[0m)                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m726\u001b[0m in \u001b[92mpipeline\u001b[0m       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m723 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mInferring the task automatically requires to check the hub with a model\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m724 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mmodel\u001b[33m}\u001b[0m\u001b[33m is not a valid model_id.\u001b[0m\u001b[33m\"\u001b[0m                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m725 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m726 \u001b[2m│   │   \u001b[0mtask = get_task(model, use_auth_token)                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m727 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m728 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Retrieve the task\u001b[0m                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m729 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m task \u001b[95min\u001b[0m custom_tasks:                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m440\u001b[0m in \u001b[92mget_task\u001b[0m       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m437 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mThe model \u001b[0m\u001b[33m{\u001b[0mmodel\u001b[33m}\u001b[0m\u001b[33m does not seem to have a correct `pipeline_tag` set to in\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m438 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m439 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mgetattr\u001b[0m(info, \u001b[33m\"\u001b[0m\u001b[33mlibrary_name\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mtransformers\u001b[0m\u001b[33m\"\u001b[0m) != \u001b[33m\"\u001b[0m\u001b[33mtransformers\u001b[0m\u001b[33m\"\u001b[0m:                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m440 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mThis model is meant to be used with \u001b[0m\u001b[33m{\u001b[0minfo.library_name\u001b[33m}\u001b[0m\u001b[33m not\u001b[0m   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m441 \u001b[0m\u001b[2m│   \u001b[0mtask = info.pipeline_tag                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m442 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m task                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m443 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mRuntimeError: \u001b[0mThis model is meant to be used with sentence-transformers not with transformers\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">transformers</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> pipeline                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 # model = SentenceTransformer('all-MiniLM-L6-v2')</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 7 model = pipeline(model=<span style=\"color: #808000; text-decoration-color: #808000\">\"sentence-transformers/all-MiniLM-L6-v2\"</span>)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 #Sentences are encoded by calling model.encode()</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>emb1 = model.encode(<span style=\"color: #808000; text-decoration-color: #808000\">\"This is a red cat with a hat.\"</span>)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">726</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">pipeline</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">723 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Inferring the task automatically requires to check the hub with a model</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">724 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>model<span style=\"color: #808000; text-decoration-color: #808000\">} is not a valid model_id.\"</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">725 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>726 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>task = get_task(model, use_auth_token)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">727 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">728 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Retrieve the task</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">729 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> task <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> custom_tasks:                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/pipelines/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">440</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_task</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">437 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"The model {</span>model<span style=\"color: #808000; text-decoration-color: #808000\">} does not seem to have a correct `pipeline_tag` set to in</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">438 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">439 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">getattr</span>(info, <span style=\"color: #808000; text-decoration-color: #808000\">\"library_name\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"transformers\"</span>) != <span style=\"color: #808000; text-decoration-color: #808000\">\"transformers\"</span>:                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>440 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"This model is meant to be used with {</span>info.library_name<span style=\"color: #808000; text-decoration-color: #808000\">} not</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">441 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>task = info.pipeline_tag                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">442 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> task                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">443 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>This model is meant to be used with sentence-transformers not with transformers\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"from langchain import PromptTemplate, LLMChain\nfrom langchain.llms import HuggingFacePipeline\n\nmodel= pipeline(model=\"google/flan-t5-large\") #'text2text-generation'\nmodel.save_pretrained(\"~/flan-t5-large\")\nllm = HuggingFacePipeline.from_model_id(model_id=\"~/flan-t5-large\", task=\"text2text-generation\", model_kwargs={\"temperature\":9})","metadata":{"execution":{"iopub.status.busy":"2023-08-15T22:16:58.555394Z","iopub.execute_input":"2023-08-15T22:16:58.555797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"template = PromptTemplate(input_variables=[\"input\"], template=\"{input}\")\nchain = LLMChain(llm=llm, verbose=True, prompt=template)\nchain(\"What is the meaning of life?\")","metadata":{"execution":{"iopub.status.busy":"2023-08-15T22:16:30.000877Z","iopub.execute_input":"2023-08-15T22:16:30.002107Z","iopub.status.idle":"2023-08-15T22:16:30.760748Z","shell.execute_reply.started":"2023-08-15T22:16:30.002061Z","shell.execute_reply":"2023-08-15T22:16:30.759775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://github.com/AndreasFischer1985/code-snippets/blob/master/py/LangChain_HuggingFace_examples.py\nfrom transformers import pipeline\nmodel= pipeline(model=\"sentence-transformers/all-MiniLM-L6-v2\") \nmodel.save_pretrained(\"~/all-MiniLM-L6-v2\")\n","metadata":{"execution":{"iopub.status.busy":"2023-08-15T21:46:15.183765Z","iopub.execute_input":"2023-08-15T21:46:15.184449Z","iopub.status.idle":"2023-08-15T21:46:33.189314Z","shell.execute_reply.started":"2023-08-15T21:46:15.184413Z","shell.execute_reply":"2023-08-15T21:46:33.187832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"HUGGING_FACE_API_KEY = \"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\"\n\nmodel = HuggingFacePipeline.from_model_id(\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    task=\"text-generation\",\n    model_kwargs={\"temperature\": 0, \"max_length\": 512},\n)\n\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\nembeddings = (\n    HuggingFaceEmbeddings(\n        model_name='sentence-transformers/all-MiniLM-L6-v2', \n        encode_kwargs={\"max_new_tokens\": 512}\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T23:10:50.176752Z","iopub.execute_input":"2023-08-14T23:10:50.177227Z","iopub.status.idle":"2023-08-14T23:10:51.134881Z","shell.execute_reply.started":"2023-08-14T23:10:50.177165Z","shell.execute_reply":"2023-08-14T23:10:51.133890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings","metadata":{"execution":{"iopub.status.busy":"2023-08-14T23:10:58.075998Z","iopub.execute_input":"2023-08-14T23:10:58.076383Z","iopub.status.idle":"2023-08-14T23:10:58.085326Z","shell.execute_reply.started":"2023-08-14T23:10:58.076349Z","shell.execute_reply":"2023-08-14T23:10:58.084108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = (\n    VectorstoreIndexCreator(\n        embedding=embeddings, \n        vectorstore_cls=Chroma,\n        text_splitter=CharacterTextSplitter(chunk_size=100,chunk_overlap=0)\n    ).from_loaders([loader])\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T23:11:24.084584Z","iopub.execute_input":"2023-08-14T23:11:24.085034Z","iopub.status.idle":"2023-08-14T23:11:25.059781Z","shell.execute_reply.started":"2023-08-14T23:11:24.084999Z","shell.execute_reply":"2023-08-14T23:11:25.056740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2023-08-14T22:59:40.157299Z","iopub.execute_input":"2023-08-14T22:59:40.157677Z","iopub.status.idle":"2023-08-14T22:59:40.164837Z","shell.execute_reply.started":"2023-08-14T22:59:40.157642Z","shell.execute_reply":"2023-08-14T22:59:40.163904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings","metadata":{"execution":{"iopub.status.busy":"2023-08-14T22:59:40.374620Z","iopub.execute_input":"2023-08-14T22:59:40.374941Z","iopub.status.idle":"2023-08-14T22:59:40.381305Z","shell.execute_reply.started":"2023-08-14T22:59:40.374913Z","shell.execute_reply":"2023-08-14T22:59:40.380223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index","metadata":{"execution":{"iopub.status.busy":"2023-08-14T22:59:40.652167Z","iopub.execute_input":"2023-08-14T22:59:40.652579Z","iopub.status.idle":"2023-08-14T22:59:40.658873Z","shell.execute_reply.started":"2023-08-14T22:59:40.652546Z","shell.execute_reply":"2023-08-14T22:59:40.657861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index.query(\"Compare chain of though vs tree of thought methods\", llm=model)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T23:02:17.015215Z","iopub.execute_input":"2023-08-14T23:02:17.015953Z","iopub.status.idle":"2023-08-14T23:02:20.125285Z","shell.execute_reply.started":"2023-08-14T23:02:17.015913Z","shell.execute_reply":"2023-08-14T23:02:20.123761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://github.com/Lightning-Universe/Comparing_LLM_Blogpost/blob/master/Comparing_LLM.py\n\nimport torch \nfrom pathlib import Path\nfrom langchain.llms.base import LLM\nfrom transformers import T5Tokenizer,T5ForConditionalGeneration\nfrom pydantic import BaseModel\nfrom typing import Optional, List\n\nclass CustomPipeline(LLM): \n    def __init__(self, model_id):\n        super().__init__()\n        global model, tokenizer, model_name\n    \n        device_map = \"auto\"\n        model_name = model_id\n        model = T5ForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device_map)\n        tokenizer = T5Tokenizer.from_pretrained(model_id)\n        \n    @property\n    def _llm_type(self) -> str:\n        return \"custom_pipeline\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None):\n        with torch.no_grad():\n            input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n            outputs = model.generate(input_ids, max_new_tokens = 70)\n            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            return response\n        ","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:57:14.063152Z","iopub.execute_input":"2023-08-16T19:57:14.063669Z","iopub.status.idle":"2023-08-16T19:57:14.078806Z","shell.execute_reply.started":"2023-08-16T19:57:14.063626Z","shell.execute_reply":"2023-08-16T19:57:14.077100Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model_id = 'google/flan-t5-xxl'\nllm = CustomPipeline(model_id)","metadata":{"execution":{"iopub.status.busy":"2023-08-16T19:57:17.236451Z","iopub.execute_input":"2023-08-16T19:57:17.236888Z","iopub.status.idle":"2023-08-16T20:08:37.925957Z","shell.execute_reply.started":"2023-08-16T19:57:17.236851Z","shell.execute_reply":"2023-08-16T20:08:37.924799Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2108fd38914df8ad265cc292736f50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)fetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac6006a5941942a99d0dff66c39f2b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdef9a6bc0bd4649befd544fbbe9afa0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00005.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c5070525d841c3a4658d7f0798d25e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00005.safetensors:   0%|          | 0.00/9.60G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72349c1fdbbe4d0197a52013f9524252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00005.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d9bd9b349644e249a8fd57c795bd8e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00005.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0b4887cf0064191b6747a0b64c6c515"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00005.safetensors:   0%|          | 0.00/6.06G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"981d72efb44d45b9a1c88396c9f28bf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9a999a9a60a434e96c3a0c34b5c65f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f08fdebec61419bab72c1be608d68c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb65e9f1ede41b5937f81e6d009b332"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5f33cf0cd3748718f4f359392bb0776"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0782aeee9c249dca1f3450efa1fa5be"}},"metadata":{}}]},{"cell_type":"code","source":"template = \"\"\"\nWe have provided context information below.\n\nElon Musk's Twitter misery seems to be delighting users on the social media platform, as he got stuck with a new screen name.\n\nUsing only this information, please answer the question: {text}\n\"\"\"\n\nprompt_template = PromptTemplate(input_variables=[ \"text\"], template=template)\nanswer_chain = LLMChain(llm=llm , prompt=prompt_template)\n\nquestions = [\"what’s Elon's new Twitter username?\",\n    \"why is it funny that he cannot change it?\",\n    \"make a joke about this\",\n    \"How did this get started?\"\n    ]\n\nfor question in questions:\n    answer_chain = LLMChain(llm=llm , prompt=prompt_template)\n    answer = answer_chain.run(question)\n    print(f\"\\nThe question is: {question }\")\n    print(f\"\\n {answer.strip()}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-16T20:11:42.296933Z","iopub.execute_input":"2023-08-16T20:11:42.297458Z","iopub.status.idle":"2023-08-16T20:33:24.109431Z","shell.execute_reply.started":"2023-08-16T20:11:42.297418Z","shell.execute_reply":"2023-08-16T20:33:24.107914Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\nThe question is: what’s Elon's new Twitter username?\n\n @elonmusk\n\nThe question is: why is it funny that he cannot change it?\n\n he got stuck with a new screen name\n\nThe question is: make a joke about this\n\n Elon Musk is a twit\n\nThe question is: How did this get started?\n\n Elon Musk got stuck with a new screen name\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}