{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Loading RoBERTa from local files \n\nThis notebook shows how to load a RoBERTa tokenizer and model from a local directory (without downloading from HF hub, or other network location). \n\n## References: \n1. [Kaggle - tokenizers cheat sheet](https://www.kaggle.com/code/debanga/huggingface-tokenizers-cheat-sheet)\n2. [Kaggle dataset with saved roberta model](https://www.kaggle.com/datasets/abhishek/roberta-base)\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-04-11T20:34:49.936667Z",
     "iopub.execute_input": "2023-04-11T20:34:49.936936Z",
     "iopub.status.idle": "2023-04-11T20:34:49.982587Z",
     "shell.execute_reply.started": "2023-04-11T20:34:49.936909Z",
     "shell.execute_reply": "2023-04-11T20:34:49.981536Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "/kaggle/input/roberta-base/rust_model.ot\n/kaggle/input/roberta-base/config.json\n/kaggle/input/roberta-base/merges.txt\n/kaggle/input/roberta-base/README.md\n/kaggle/input/roberta-base/tokenizer.json\n/kaggle/input/roberta-base/vocab.json\n/kaggle/input/roberta-base/tf_model.h5\n/kaggle/input/roberta-base/dict.txt\n/kaggle/input/roberta-base/pytorch_model.bin\n/kaggle/input/roberta-base/flax_model.msgpack\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import transformers\nimport torch\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\nimport tokenizers ",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:42:34.339825Z",
     "iopub.execute_input": "2023-04-11T20:42:34.340899Z",
     "iopub.status.idle": "2023-04-11T20:42:34.346830Z",
     "shell.execute_reply.started": "2023-04-11T20:42:34.340858Z",
     "shell.execute_reply": "2023-04-11T20:42:34.345660Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Method 1 (not preferred)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "ROBERTA_PATH = '../input/roberta-base'\n\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab=f'{ROBERTA_PATH}/vocab.json', \n    merges=f'{ROBERTA_PATH}/merges.txt', \n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:35:06.339240Z",
     "iopub.execute_input": "2023-04-11T20:35:06.340473Z",
     "iopub.status.idle": "2023-04-11T20:35:06.468766Z",
     "shell.execute_reply.started": "2023-04-11T20:35:06.340434Z",
     "shell.execute_reply": "2023-04-11T20:35:06.467447Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "tokenizer",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:35:21.433257Z",
     "iopub.execute_input": "2023-04-11T20:35:21.434287Z",
     "iopub.status.idle": "2023-04-11T20:35:21.444434Z",
     "shell.execute_reply.started": "2023-04-11T20:35:21.434233Z",
     "shell.execute_reply": "2023-04-11T20:35:21.443303Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "Tokenizer(vocabulary_size=50265, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "text = 'Today is Monday. Long live Monday. Monday is before Tuesday. Tokenizing, eh?'\ntext = text.lower()\n\ntext_encoded = tokenizer.encode(text)\ntext_encoded",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:35:57.913941Z",
     "iopub.execute_input": "2023-04-11T20:35:57.914944Z",
     "iopub.status.idle": "2023-04-11T20:35:57.925140Z",
     "shell.execute_reply.started": "2023-04-11T20:35:57.914879Z",
     "shell.execute_reply": "2023-04-11T20:35:57.923735Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "Encoding(num_tokens=22, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "text_encoded.tokens",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:36:02.461938Z",
     "iopub.execute_input": "2023-04-11T20:36:02.462939Z",
     "iopub.status.idle": "2023-04-11T20:36:02.470951Z",
     "shell.execute_reply.started": "2023-04-11T20:36:02.462902Z",
     "shell.execute_reply": "2023-04-11T20:36:02.469854Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['today',\n 'Ġis',\n 'Ġm',\n 'onday',\n '.',\n 'Ġlong',\n 'Ġlive',\n 'Ġm',\n 'onday',\n '.',\n 'Ġm',\n 'onday',\n 'Ġis',\n 'Ġbefore',\n 'Ġt',\n 'uesday',\n '.',\n 'Ġtoken',\n 'izing',\n ',',\n 'Ġeh',\n '?']"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "text_encoded.ids",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:36:08.235021Z",
     "iopub.execute_input": "2023-04-11T20:36:08.235749Z",
     "iopub.status.idle": "2023-04-11T20:36:08.243302Z",
     "shell.execute_reply.started": "2023-04-11T20:36:08.235693Z",
     "shell.execute_reply": "2023-04-11T20:36:08.241969Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "execution_count": 10,
     "output_type": "execute_result",
     "data": {
      "text/plain": "[34375,\n 16,\n 475,\n 46328,\n 4,\n 251,\n 697,\n 475,\n 46328,\n 4,\n 475,\n 46328,\n 16,\n 137,\n 326,\n 47478,\n 4,\n 19233,\n 2787,\n 6,\n 35670,\n 116]"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Method 2 (Preferred)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Loading tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=ROBERTA_PATH)\ntokenizer",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:36:13.464927Z",
     "iopub.execute_input": "2023-04-11T20:36:13.465695Z",
     "iopub.status.idle": "2023-04-11T20:36:13.651325Z",
     "shell.execute_reply.started": "2023-04-11T20:36:13.465654Z",
     "shell.execute_reply": "2023-04-11T20:36:13.650076Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "RobertaTokenizerFast(name_or_path='../input/roberta-base', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "text = ['Today is Monday. Long live Monday. Monday is before Tuesday', 'Tokenizing, eh?']\ntext = [txt.lower() for txt in text]\ntext_encoded = tokenizer(text, padding=True)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:36:57.518561Z",
     "iopub.execute_input": "2023-04-11T20:36:57.519208Z",
     "iopub.status.idle": "2023-04-11T20:36:57.527992Z",
     "shell.execute_reply.started": "2023-04-11T20:36:57.519166Z",
     "shell.execute_reply": "2023-04-11T20:36:57.526711Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "text_encoded",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:36:58.547318Z",
     "iopub.execute_input": "2023-04-11T20:36:58.547709Z",
     "iopub.status.idle": "2023-04-11T20:36:58.555029Z",
     "shell.execute_reply.started": "2023-04-11T20:36:58.547673Z",
     "shell.execute_reply": "2023-04-11T20:36:58.553743Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "execution_count": 14,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'input_ids': [[0, 34375, 16, 475, 46328, 4, 251, 697, 475, 46328, 4, 475, 46328, 16, 137, 326, 47478, 2], [0, 46657, 2787, 6, 35670, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "for txt in [0,1]: \n    tokens = tokenizer.convert_ids_to_tokens(text_encoded.input_ids[txt])\n    print(tokens)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:39:13.953395Z",
     "iopub.execute_input": "2023-04-11T20:39:13.953798Z",
     "iopub.status.idle": "2023-04-11T20:39:13.961220Z",
     "shell.execute_reply.started": "2023-04-11T20:39:13.953762Z",
     "shell.execute_reply": "2023-04-11T20:39:13.959812Z"
    },
    "trusted": true
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": "['<s>', 'today', 'Ġis', 'Ġm', 'onday', '.', 'Ġlong', 'Ġlive', 'Ġm', 'onday', '.', 'Ġm', 'onday', 'Ġis', 'Ġbefore', 'Ġt', 'uesday', '</s>']\n['<s>', 'token', 'izing', ',', 'Ġeh', '?', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "model_inputs = tokenizer(text, return_tensors='pt', padding=True)\nmodel_inputs",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:39:17.397624Z",
     "iopub.execute_input": "2023-04-11T20:39:17.398028Z",
     "iopub.status.idle": "2023-04-11T20:39:20.035188Z",
     "shell.execute_reply.started": "2023-04-11T20:39:17.397969Z",
     "shell.execute_reply": "2023-04-11T20:39:20.034015Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "execution_count": 20,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'input_ids': tensor([[    0, 34375,    16,   475, 46328,     4,   251,   697,   475, 46328,\n             4,   475, 46328,    16,   137,   326, 47478,     2],\n        [    0, 46657,  2787,     6, 35670,   116,     2,     1,     1,     1,\n             1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Loading model for classification",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=ROBERTA_PATH, num_labels=4)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:39:23.053276Z",
     "iopub.execute_input": "2023-04-11T20:39:23.053761Z",
     "iopub.status.idle": "2023-04-11T20:39:29.901181Z",
     "shell.execute_reply.started": "2023-04-11T20:39:23.053724Z",
     "shell.execute_reply": "2023-04-11T20:39:29.900037Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Some weights of the model checkpoint at ../input/roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ../input/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "model",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:39:29.903953Z",
     "iopub.execute_input": "2023-04-11T20:39:29.904491Z",
     "iopub.status.idle": "2023-04-11T20:39:29.914838Z",
     "shell.execute_reply.started": "2023-04-11T20:39:29.904445Z",
     "shell.execute_reply": "2023-04-11T20:39:29.913812Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": [
    {
     "execution_count": 22,
     "output_type": "execute_result",
     "data": {
      "text/plain": "RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n  )\n)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "model.config",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:39:30.838290Z",
     "iopub.execute_input": "2023-04-11T20:39:30.838934Z",
     "iopub.status.idle": "2023-04-11T20:39:30.847341Z",
     "shell.execute_reply.started": "2023-04-11T20:39:30.838893Z",
     "shell.execute_reply": "2023-04-11T20:39:30.845973Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": [
    {
     "execution_count": 23,
     "output_type": "execute_result",
     "data": {
      "text/plain": "RobertaConfig {\n  \"_name_or_path\": \"../input/roberta-base\",\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3\n  },\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.26.1\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 50265\n}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "weights = model.state_dict()\nweights.keys()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:39:36.917090Z",
     "iopub.execute_input": "2023-04-11T20:39:36.917978Z",
     "iopub.status.idle": "2023-04-11T20:39:36.929514Z",
     "shell.execute_reply.started": "2023-04-11T20:39:36.917939Z",
     "shell.execute_reply": "2023-04-11T20:39:36.928319Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "execution_count": 24,
     "output_type": "execute_result",
     "data": {
      "text/plain": "odict_keys(['roberta.embeddings.position_ids', 'roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias'])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "model.eval()\nwith torch.no_grad(): \n    classifier_output = model(**model_inputs)\n    print(classifier_output)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:41:06.027614Z",
     "iopub.execute_input": "2023-04-11T20:41:06.028906Z",
     "iopub.status.idle": "2023-04-11T20:41:06.169239Z",
     "shell.execute_reply.started": "2023-04-11T20:41:06.028856Z",
     "shell.execute_reply": "2023-04-11T20:41:06.167965Z"
    },
    "trusted": true
   },
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "text": "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.0335, -0.0112,  0.0244,  0.0595],\n        [ 0.0372, -0.0014,  0.0135,  0.0549]]), hidden_states=None, attentions=None)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Loading model for embedding (no classifier head) ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "model_02 = AutoModel.from_pretrained(pretrained_model_name_or_path=ROBERTA_PATH)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:45:02.460857Z",
     "iopub.execute_input": "2023-04-11T20:45:02.461622Z",
     "iopub.status.idle": "2023-04-11T20:45:04.128610Z",
     "shell.execute_reply.started": "2023-04-11T20:45:02.461582Z",
     "shell.execute_reply": "2023-04-11T20:45:04.127499Z"
    },
    "trusted": true
   },
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "text": "Some weights of the model checkpoint at ../input/roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "model_02.eval()\nwith torch.no_grad(): \n    cls_token_embedding = model_02(**model_inputs).last_hidden_state[:,0,:]\n\ncls_token_embedding",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:46:06.491875Z",
     "iopub.execute_input": "2023-04-11T20:46:06.492520Z",
     "iopub.status.idle": "2023-04-11T20:46:06.648301Z",
     "shell.execute_reply.started": "2023-04-11T20:46:06.492476Z",
     "shell.execute_reply": "2023-04-11T20:46:06.646978Z"
    },
    "trusted": true
   },
   "execution_count": 40,
   "outputs": [
    {
     "execution_count": 40,
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.0777,  0.0887,  0.0029,  ..., -0.1063, -0.0391,  0.0045],\n        [-0.0523,  0.0936, -0.0232,  ..., -0.0355, -0.0570, -0.0413]])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "cls_token_embedding.shape",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-04-11T20:46:17.830190Z",
     "iopub.execute_input": "2023-04-11T20:46:17.830673Z",
     "iopub.status.idle": "2023-04-11T20:46:17.841782Z",
     "shell.execute_reply.started": "2023-04-11T20:46:17.830615Z",
     "shell.execute_reply": "2023-04-11T20:46:17.840581Z"
    },
    "trusted": true
   },
   "execution_count": 41,
   "outputs": [
    {
     "execution_count": 41,
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([2, 768])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}